# -*- coding: utf-8 -*-
"""UniAs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-5C6xa6whZQIEWqo3BkL34q4qw-9y8jg
"""

# pip install langgraph langchain-community langchain_openai

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph langchain_core

import os
from flask import Flask, request, jsonify
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import time
# from langchain.prompts import HumanMessage
# from langchain import HumanMessage

import os

app = Flask(__name__)

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "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"

from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "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"
model = ChatOpenAI(model="gpt-3.5-turbo")

from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# Define the dictionary of services as before
# Define the dictionary of services with image paths as values for final keys
pnb_services = {
    "PNB_ONE": {

        "Fund Transfer": {
            "transfer  Funds": "login/homepage/funds_transfer.jpg"
        },

        "Money Transfer via UPI":{
            "Send money via upi" : "login/homepage/upi/send_money.jpg"
        }
    }
}

from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    ToolMessage,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langgraph.graph import END, StateGraph, START



from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

def create_overview_agent(llm):
    """Create an agent to interpret user intent and identify main tasks."""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a Bank assistant, an expert assistant collaborating with other assistants for customers seeking guidance with bank services. "
                "Your role is to help customers by clearly identifying the main purposes or intentions in their queries. "
                "You are here to ease their journey by extracting key tasks and routing any needed work to other assistants. "
                "You also respond politely to simple questions, such as 'How are you?' or 'What can you do?'."
            ),
            (
                "user",
                "query"
            ),
            (
                "assistant",
                "If the user query is casual, such as asking about my status or abilities, respond with a friendly answer. "
                "Otherwise, analyze the user’s query and break down the main tasks for further processing. "
                "Use this format for detailed tasks:\n\n"
                "- [Bank Name]: Extract the bank's name, if mentioned.\n"
                "- [Service/App Name]: Identify the bank service or app mentioned by the user.\n"
                "- [Main Action/Intention]: Identify the user's main action or goal, such as applying for a loan, checking a balance, etc.\n"
                "- [Original Query] : 'query received from user'. \n\n "
                "Respond in a structured format as follows:\n\n"
                "1. Greeting or basic response (if a casual question).\n"
                "2. Key breakdown if it’s a service query.\n\n"
                "Example response for service queries:\n\n"
                "Structured Information:\n"
                "- Bank Name: PNB\n"
                "- Service/App: PNB ONE\n"
                "- Main Action: Apply for a Loan\n\n"
                "- Original Query : 'query received from user'"
                "Proceed to pass this structured information to the next assistant for further steps."
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    return prompt | llm

# Define the prompt template for the agent
def create_service_path_agent(llm, pnb_services):
    """
    Creates an agent that searches through the nested dictionary of services to find the exact image path based on the user's query.

    Args:
        llm: The language model instance.
        pnb_services: The dictionary structure containing service names and image paths.
        detailed information : analyze Structured Information of the customer task received from previous assistant.
        Original Query : Received from previous assistant.
    Returns:
        A chat prompt template for finding the specific path in the nested dictionary.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a banking AI Assistant collaborating with other assistants that guides users by providing exact steps to complete tasks within the PNB ONE application."
                "You have access to a structured dictionary of services ('pnb_services') where each service corresponds to an image path."
                "Your goal is to identify the service path based on the structured information of the query/task received from previous assitant"
                 "and return the corresponding image path."
                "\n\n"
                "pnb_services: {pnb_services}\n"
                "For each query, carefully match keywords in the dictionary to determine the exact location."
                "Return the path to the image as the response."
                "\nIf you find the image path based on the information received from previous assistant, respond with the path only. "
                "If not, use your internal knowledge to find the nearest service path for the task.  "
            ),
            (
                "ai",
                "User Query: {{user_query}}\n"
                "Clarified Intent: {{clarified_intent}}\n"
                "Example:\n"
                "User Query: 'I want to reset my MPIN in the PNB ONE app.'\n"
                "Clarified Intent: 'PNB ONE / Password / RESET / MPIN'\n"
                "Service Path: 'login/homepage/homepage2/passwordreset/mpinreset.jpg',"
                "Original query: 'Original Query' you received from previous assistant."
                "You should return the image path and original query as shown in the example."
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )

    prompt = prompt.partial(pnb_services=pnb_services)
    return prompt | llm

image_metadata = {



    # Login
    "login.jpg": {
        "description": "user login page.",
        "elements": ["Enter MPIN"],
        "actions": ["Enter you MPIN", "Click on 'Login'"]
    },

     #homepage
    "homepage.jpg": {
        "description": "search for the icon  of the service or task to reach the next page ",
        "elements": ["Fund Transfer icon ", "UPI icon" " Scroll the page"],
        "actions": ["Look for the icon ", "click on the icon ","Scroll down and search for the icon or text"]
    },




    # Fund Transfer
    "funds_transfer.jpg": {
        "description": "Options to transfer funds",
        "elements": ["Transfer Funds to Beneficiary option", "Quick fund Transfer to Beneficiary option", "Transfer Funds to self option"],
        "actions": ["select the option of Transfer Funds to Beneficiary from the below option", "select the option of Quick Transfer Funds  to Beneficiary from the below option"]

    },

    # "transfer_funds_to_beneficiary.jpg" :{
    #    "description": "Transfer funds  to beneficiary final step",
    #     "elements": ["select the beneficiary", "Continue button"],
    #     "actions": ["select a beneficiary from below","Transfer Money"]


    

    #Money transfer via UPI 

    "upi.jpg": {
        "description": "UPI money transfer page.",
        "elements": ["Pay to Mobile No", "Generate QR code"],
        "actions": ["Select the option of Pay to Mobile No"]
    },

    "send_money.jpg":{
        "description": "details to send money via upi",
        "elements": ["Enter UPI ID field", "Enter Amount field", "continue button", "Transfer Button"],
        "actions": ["Enter the UPI ID", "Enter the Remarks", "Click on 'Continue'"]
    
    },

}


def create_final_output_agent(llm, image_metadata):
    """
    Generates an agent prompt template that guides the user through each image in the path with detailed instructions,
    in a dictionary format. The instructions are generated based on metadata guidance but are interpreted and dynamically
    explained by the assistant.

    Args:
        llm: The language model instance.
        image_metadata: Dictionary containing metadata about each image and associated actions.

    Returns:
        A chat prompt template for generating dynamic, action-based responses in a dictionary format.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a highly knowledgeable and detailed banking assistant, tasked with guiding users through each step "
                "in their journey. For each image, use the information in image_metadata to interpret and explain the actions "
                "users need to take in a personalized, clear, and supportive manner. The final response should be formatted as a dictionary, "
                "where each dictionary key is the image name (e.g., 'login.jpg'), and the value is a detailed, natural-language "
                "explanation of the action required."
                "\n\n"
                "Each step should be explained as though you are speaking directly to the user. For example,if the metadata suggests "
                "actions like 'enter PIN' or 'click login', you might say:\n\n"
                "{{ 'login.jpg': 'First, go to the login page. Here, carefully enter your PIN in the designated field and click on the Login button to access the homepage.' }}\n\n"
                "Another example might be:\n\n"
                "{{ 'homepage.jpg': 'Now that you are on the homepage, scroll down until you see the password reset icon. Tap on it to proceed to the password reset options.' }}\n\n"
                "Using image_metadata, provide detailed and helpful instructions for each image in service_path, in a dictionary format, interpreting actions dynamically as shown."
            ),
            (
                "user",
                "User Query:{{Original Query}}\n"
                "Clarified Intent: {{clarified_intent}}\n"
                "Service Path: Image path received from the previous assistant.\n\n"
                "Please analyze image_metadata and use it as a reference to generate step-by-step guidance in dictionary format for each image in service_path, "
                "offering dynamic and descriptive instructions."
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt.partial(image_metadata=image_metadata)
    return prompt | llm

import operator
from typing import Annotated, Sequence
from typing_extensions import TypedDict

from langchain_openai import ChatOpenAI


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    sender: str
    original_query: str

import functools

from langchain_core.messages import AIMessage


# Helper function to create a node for a given agent
def agent_node(state, agent, name):
    result = agent.invoke(state)
    # We convert the agent output into a format that is suitable to append to the global state
    if isinstance(result, ToolMessage):
        pass
    else:
        result = AIMessage(**result.dict(exclude={"type", "name"}), name=name)
    return {
        "messages": [result],
        # Since we have a strict workflow, we can
        # track the sender so we know who to pass to next.
        "sender": name,
    }


llm = ChatOpenAI(model="gpt-3.5-turbo")

workflow = StateGraph(AgentState)

overview_agent = create_overview_agent(llm)
agentnode_1= functools.partial(agent_node,agent= overview_agent, name="overview_agent")

service_path_agent= create_service_path_agent(llm,pnb_services)
agentnode_2= functools.partial(agent_node,agent= service_path_agent,name= "service_path_agent")

final_agent= create_final_output_agent(llm,image_metadata)
agentnode_3= functools.partial(agent_node,agent= final_agent,name= "final_agent")

workflow = StateGraph(AgentState)

workflow.add_node("overviewer", agentnode_1)
workflow.add_node("pather", agentnode_2)
workflow.add_node("finaliser", agentnode_3)

workflow.add_edge("overviewer","pather")
workflow.add_edge("pather","finaliser")

workflow.add_edge(START, "overviewer")
graph = workflow.compile()

# events = graph.stream(
#     {
#         "messages": [
#             HumanMessage(
#                 content="I want to get a new debit card for my bank account through pnb one app"
#             )
#         ],
#     },
#     # Maximum number of steps to take in the graph
#     {"recursion_limit": 150},
# )
# for s in events:
#     print(s)
#     print("----")




# from IPython.display import Image, display

# try:
#     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
# except Exception:
#     # This requires some extra dependencies and is optional
#     pass

from flask import Flask, request, jsonify# Import the model
from langchain.schema import AIMessage
import re  # For pattern matching

app = Flask(__name__)

@app.route('/process_query', methods=['POST'])
def process_query_route():
    # Extract query from the JSON request
    data = request.json
    user_query = data.get("query")

    # Run the model and get the extracted response
    new_response = process_query(user_query)

    # Return the response as JSON
    return jsonify({"response": new_response})


def process_query(user_query):
    # Run the model with the query and stream the events
    events = graph.stream(
        {
            "messages": [
                HumanMessage(content=user_query)
            ],
        },
        {"recursion_limit": 150}
    )

    # Initialize the variable to store the extracted response
    new_response = None

    # Pattern to match keys like 'login.jpg', 'homepage.jpg', etc.
    image_pattern = re.compile(r".+\.jpg")

    # Iterate through events and extract the 'finaliser' content
    for event in events:
        # Check if 'finaliser' exists in the event
        if 'finaliser' in event:
            # Extract the content from 'AIMessage' under 'messages'
            finaliser_messages = event['finaliser']['messages']
            for message in finaliser_messages:
                if isinstance(message, AIMessage) or hasattr(message, 'content'):
                    try:
                        # Convert the content string into a dictionary (if possible)
                        finaliser_content = eval(message.content)

                        # Extract only the key-value pairs where the key matches the image pattern (like 'login.jpg')
                        new_response = {
                            key: value
                            for key, value in finaliser_content.items()
                            if image_pattern.match(key)
                        }
                    except Exception as e:
                        print(f"Error parsing finaliser content: {e}")
                    break  # Stop after extracting the finaliser content

    # Log or return the extracted response
    if new_response:
        print("Extracted Finaliser Response:", new_response)
    else:
        print("No finaliser response found.")
    
    return new_response


if __name__ == "__main__":
    app.run(port=5000)


# @app.route('/process_query', methods=['POST'])
# def process_query():
#     # Extract query from the JSON request
#     data = request.json
#     user_query = data.get("query")

    # Run the model with the query and stream the events
    # events = graph.stream(
    #     {
    #         "messages": [
    #             HumanMessage(content=user_query)
    #         ],
    #     },
    #     {"recursion_limit": 150}
    # )
    
    # Parse events to extract useful data
    # events_list = []
    # for event in events:

    #     # Extract specific attributes (modify based on your actual JSON structure)
    #     response_data = {
    #         "type": event.get("type"),
    #         "content": event.get("content") if "content" in event else None,
    #         # Add more fields as necessary from your nested JSON structure
    #     }
    #     events_list.append(response_data)

    # Structure final JSON response
    # responses =[]
    # for s in events :
    #     responses.append(s)
    # result = {"response": responses}
    # return jsonify(result)

    # # Log type and content for debugging
    # print(type(result))
    # print(result)

    # # Return as JSON response
    # return jsonify(result)
